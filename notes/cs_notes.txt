Course 1: Introduction to DRL
-----------------------------
1.8.3 Monte Carlo Methods
1.8.17 BlackJackEnv
http://localhost:8888/notebooks/monte-carlo/Monte_Carlo.ipynb

1.9: Tremporal-Difference
9.4: Sarsa
9.6: Q-Learning
9.8: Expected Sarsa
9.13 Exercise
http://localhost:8888/notebooks/temporal-difference/Temporal_Difference.ipynb


Difference between MC and TD
---------------------------
Monte Carlo (MC) Methods

How it works: Learn from complete episodes. You generate an episode (state â†’ action â†’ â€¦ â†’ reward until terminal state), then use the *actual returns* from that episode to update the value function.
Update rule:

  $$
  V(s) \leftarrow V(s) + \alpha \big( G_t - V(s) \big)
  $$

  where $G_t$ is the total return (sum of discounted rewards) after visiting state $s$.
Characteristics:
  * Requires waiting until the end of an episode (no updates mid-episode).
  * Unbiased estimates (true averages of returns).
  * Higher variance since it depends on full trajectories.
Best used when: Episodes are short and finite, e.g., board games (chess, Go) or episodic tasks.

---

Temporal-Difference (TD) Methods

How it works: Learn from incomplete episodes. Updates happen after every step by â€œbootstrappingâ€ â€” i.e., using the estimated value of the next state rather than waiting for the final outcome.
Update rule (TD(0)):

  $$
  V(s) \leftarrow V(s) + \alpha \big( r + \gamma V(s') - V(s) \big)
  $$

  where $r$ is the immediate reward and $V(s')$ is the estimated value of the next state.
Characteristics:
  * Updates online, step by step.
  * Biased (since it uses estimates of $V(s')$), but usually much lower variance.
  * Works in continuing tasks (no need for terminal states).
Best used when: You need online learning, long/continuous tasks, or when waiting for full episodes is impractical.

---

Key Differences

| Feature       | Monte Carlo        | Temporal Difference                     |
| ------------- | ------------------ | --------------------------------------- |
| Update timing | After full episode | After each step                         |
| Data used     | Actual returns     | Immediate reward + estimated next value |
| Variance      | High               | Low                                     |
| Bias          | None (unbiased)    | Biased                                  |
| Suitability   | Episodic tasks     | Continuous/online tasks                 |

---

Intuition

* Monte Carlo = â€œlearn from the actual score after the whole gameâ€.
* TD = â€œlearn by predicting the score as the game unfolds, using your current estimates.â€

Course 2: Value based methods
-----------------------------
2.2 Deep Q-Networks
It's easy to write an agent to play pong perfectly if you have the state space, action and so on. But this agent was given access only to pixel data like a human would see and learnt to play a whole bunch of atari games from scratch.
This agent uses a Deep Q-Network.

It has a deep neural network which acts like a function approximator. You pass in images from the video game one screen at a time, and it produces a vector of action values with max value indicating the action to take. As a reinforcement learning signal, it is fed back the change in game score at each time step. In the beginning when the neural network is initialized with random values, the actions taken are all over the place. But over time it associates the situations and sequences in the game with appropriate actions and learns to play the game well.

Convert the image to gray scale (84 x 84 pixels) and convert it to square images. They stacked four such frames to provide the sequence. State space size becomes 84 x 84 x 4. On the output side, the Q-value for every possible action in a single forward pass. Without this you would have to run this network for every action. Use this vector to take the action either stochastically, or by choosing the one with maximum value.

The screen images are first processed by convolution layers to exploit spatial relationships, and spatial rule space. As 4 layers were stacked, some temporal properties across those frames were also learnt. It used ReLU between Fully connected hidden layers. The last layer was a fully connected linear output layer which had a vector of action values. The same layer was used for different Atari games.

Training such a network requires a lot of days and the learning is not guaranteed to converge on the optimal value function. The network weights can oscillate and diverge, due to high correlation between actions and states. This can result in a very unstable and ineffective policy. In order to overcome these challenges there were several techniques and slightly modified the basic Q learning algorithm.

Training techniques
- Experience Replay
  It was proposed to make more efficient use of observed experiences. In an online Q-learning algorithm where we interact with the environment at each step and obtain a state-action-reward tuple, learn from it and then discard. Moving on to the next tuple in the following timestep. This seems wasteful. We could possibly learn more from these experienced tuples if we stored them somewhere. Moreover, some states are pretty rare to come by and some actions can be pretty costly, so it would be nice to recall such experiences. That's exactly what a replay buffer allows us to do. We store each experienced tuple in this buffer as we are interacting with the environment and then sample a small batch of tuples from it in order to learn. As a result, we are able to learn from individual tuples multiple times, recall rare occurrences, and in general make better use of our experience.
    
There is another critical problem that experience replay can help with and this is what DQN takes advantage of. If you think about the experiences being obtained, you will realize that every action At affects the next state in some way, which means that a sequence of experienced tuples can be highly correlated. A naive Q-learning approach that learns from each of these experiences in sequential order runs the risk of getting swayed by the effects of this correlation. With experience replay, can sample from this buffer at random. It doesn't have to be in the same sequence as we stored the tuples. This helps break the correlation and ultimately prevents action values from oscillating or diverging catastrophically.  

This approach is basically building a database of samples and then learning a mapping from them. In that sense experience replay helps reduce the reinforcement learning problem or at least value learning portion of it to a supervised learning scenario. We can then apply other models, learning techniques and best practices in the supervised learning literature through reinforcement learning. We can even improve upon the idea, for example, prioritizing experience tuples that are rare or more important.

- Fixed Q Targets
Experience replay helps us address one type of correlation. That is between consecutive experience tuples. There is another kind of correlation that Q-learning is susceptible to. Q-learning is a form of Temporal Difference or TD learning.

2.2.5 Deep Q-Learning Algorithm
There are two main modules. One is where we sample the environment by sampling the environment and store the observed experienced tuples in a replay memory. The other is where we select the small batch of tuples from this memory, randomly, and learn from that batch using a gradient descent update step.
These two processes are not directly dependent on each other. So, you could perform multiple sampling steps then on learning step, or even multiple learning steps with different random batches. The rest of the algorithm is designed to support these steps. In the beginning you need to initialize an empty replay memory D. Note that memory is finite, so you may want to use something like a circular Q that retains the N most recent experience tuples. Then, you also need to initialize the parameters or weights of your neural network. 

2.2.9 Double DQN
The First problem we are going to address, is the over estimation of action values that Q-learning is prone to. Let's look at the update rule of Q-learning with function approximation, and focus on the TD target. Here the max operation is necessary to find the best possible value we could get from the next state. To understand this better, let's rewrite the target and expand the max operation. It is just a more efficient way of saying that we want to obtain the Q-value for state S', and the action that results in the maximum Q-value among all possible actions from that state. When we write it this way, we can see that the argmax function makes mistakes, especially in the early stages. Why? Because the Q-values are still evolving, and we may not have gathered enough information to figure out the best action. The accuracy of the Q-values depends a lot on what actions have been tried, and what neighboring states have been explored. In fact, it has been shown that this results in an overestimation of Q-values, since we always pick the maximum among a set of noisy numbers. So, maybe we shouldn't blindly trust these values. What can we do to make our estimation more robust? One idea that has been shown to work very well in practice is called Double Q-Learning, where we select the best action using one set of parameters w, but evaluate it using a different set of parameters w'. It's basically like having two separate function approximators that must agree on the best action. If w picks an action that is not the best according to w', then the Q-value returned is not that high. In the lone run this prevents the algorithm from propagating incidental high rewards that may have been obtained by chance, and don't reflect long term returns. Now you may be thinking, where do we get the second set of parameters from? In the original formulation of Double Q-Learning, you would basically maintain two value functions, and randomly choose one of them to update at each step using the other only for evaluating actions. But when using DQNs with fixed Q targets, we already have an alternate set of parameters. Remember w_? It turns out that since w_ is kept frozen for a while, it is different enough from w that it can be reused for this purpose. And that's it, this simple modification keeps Q-values in check, preventing them from exploding in early stages of learning or fluctuating later on. The resulting policies have also been shown to perform significantly better than vanilla DQNs.  

2.2.10 Prioritized Experience Replay
We interact with the environment to collect experience tuples, save them in a buffer, and then later, we randomly sample a batch to learn from. This helps us break the correlation between consecutive experiences and stabilizes our learning algorithm. But some of these experiences may be more important for learning than others. Moreover, these important experiences might occur infrequently. If we sample the batches uniformly, then these experiences have a very small chance of getting selected. Since buffers are practically limited in capacity, older important experiences may get lost. This is where the idea of prioritized experience replay comes in. But what criteria should we use to assign priorities to each tuple? One approach is to use the TD error delta. the bigger the error the more we expect to learn from the tuple. So, let's take the magnitude of this error as a measure of priority and store it along with each corresponding tuple in the replay buffer. When creating batches, we can use this value to compute a sampling probability. Select any tuple i with a probability equal to its priority value PI, normalized by the sum of all priority values in the replay buffer. When a tuple is picked, we can update its priority with a newly computed TD error using the latest Q values. This seems to work fairly well and has been shown to reduce the number of batch updates needed to learn a value function. There are a couple of things we can improve. First, note that if the TD error is zero, then the priority value of the tuple and hence its probability of being picked will also be zero. Zero or very low TD error doesn't necessarily mean we have nothing more to learn from such a tuple, it might be the case that our estimate was closed due to the limited samples we
visited till that point. So, to prevent such tuples from being starved from selection, we can add a small constant e to every priority value. Another issue along similar lines is that greedily using these priority values may lead to a small subset of experiences being replayed over and over resulting in an overfitting to that subset. To avoid this, we can reintroduce some element of uniform random sampling. This adds another hyperparameter "A" which we use to redefine the sampling probability as, priority Pi^A / sum of all priorities Pk, each raised to the power A. We can control how much we want to use priorities versus randomness by varying this parameter. Where A equal zero corresponds to pure uniform randomness and A equals one only uses priorities. When we use prioritized experience replay, we have to make one adjustment to our update rule. Remember that our original Q learning update is derived from an expectation over all experiences. When using a stochastic update rule, the way we sample their experiences must match the underlying distribution they came from. This is preserved when we sample experience tuples uniformly from the replay buffer, but this assumption is violated when we use a non-uniform sampling, for example, using priorities. The Q values we learn will be biased according to these priority values which we only wanted to use for sampling. To correct for his bias, we need to introduce an important sampling weight equal to, 1/n where n is the size of the replay buffer, times 1/ sampling probability Pi. We can add another hyperparameter B and raise each important sampling weight to B, to control how much these weights affect learning. In fact, these weights are more important towards the end of learning when your q values begin to converge. So, you can increase B from a low value to one over time. 

2.2.11 Dueling DQN
The core idea of dueling networks is to use two streams. One that estimates the state value function V(S) and one that estimates the advantage for each action A(s,a). These streams may share some layers in the beginning such as convolution layers, then branch off with their own fully-connected layers. Finally, the desired Q values are obtained by combining the state and advantage values. The intuition behind this is that the value of most states don't vary a lot across actions. So, it makes sense to try and directly estimate them, but we still need to capture the difference actions make in each state. This is where the advantage function comes in. Some modifications are necessary to adapt Q learning to this architecture, as in the dueling network paper. Along with double DQN and prioritized replay, this technique has resulted in significant improvement over vanilla DQNs. 

Course 3: Policy based methods
-----------------------------
3.2.1
Interaction -> Optimal Value function q* -> Optimal Policy pi*

RL is about learning optimal policy from interaction with the environment. So far we have been looking at value-base methods, where we first tried to find an estimate of the optimal action value function. 

For small state spaces, this optimal value function was represented in a table with one row for each state and one column for each action. Then we use the table to build the optimal policy one state at a time. For each state, we just pull its corresponding row from the table and the optimal action is just the action with the largest entry. But what about environments with much larger state spaces? Without any discretization it's impossible to represent the optimal action value function in a table. So we investigated how to represent the optimal action value function with a neural network which formed the basis for the deep Q learning algorithm. In this case, the neural network took the environment state as input. As output it returned the value of each possible action.  

In either case we first estimate the Optimal Value function before we could tackle the optimal policy. 
Can we find the optimal policy without worrying about the Value function at all? The answer is yes. This is done using policy-based methods.


3.2.5 Hill Climbing Pseudocode

1) Initialize the weights theta in the policy arbitrarily. 
2) Collect an episode with theta, and record the return G.
theta_best = theta, G_best = G

Repeat until environment solved:
    3) Add a little bit of random noise to theta_best, to get a new set of weights theta_new.
    4) Collect an episode with theta_new and record the return G_new.
    5) If G_new > G_best then:
        theta_best = theta_new, G_best = G_new

3.2.6 Beyond Hill Climbing
Stochastic Policy Search
Steepest Ascent Hill Climbing: Generate a few candidate policies by perturbing the parameters randomly and evaluate each policy by interaction with the environment. This gives us an idea of the neighborhood of the current policy. Pick the candidate policy that looks most promising and iterate. This variation is known as steepest ascent hill climbing and it helps reduce the risk of selecting a new policy that may lead to suboptimal solutions. You could still get stuck at local optima. But there are some modifications that can help mitigate that, like, random restarts or simulated annealing.

Simulated Annealing, uses a predefined schedule to control how the policy space is explored. Starting with a large noise parameter, there is a broad neighborhood to explore, we gradually reduce the noise or radius as we get close to the optimal solution. 

Adaptive Noise Scaling, Whenever we find a better policy than before, we are likely getting closer to the optimal policy. So, it makes sense to reduce our search radius for generating the next policy. This translates to reducing or decaying the variance of the gaussian noise we add. So far it's just like simulated annealing. But, if we don't find a better policy, it's probably a good idea to increase our search radius and continue exploring from the current best policy. This small tweak to stochastic policy search makes it much less likely to get stuck, especially in domains with complicated object function.

3.2.7 Black Box optimization
Cross-Entropy method: Average of top 10% or 20% policy with highest returns in Adaptive Noise Scaling. Another approach is to look at the return that was collected by each candidate policy. The best policy will be a weighted sum of all of these, where policies that got higher return, are given more say or get  higher weight. This technique is called Evolution Strategies. 

3.2.8 Coding exercises
http://localhost:8888/notebooks/cross-entropy/CEM.ipynb



3.2.11 Why Policy-Based methods?
1) Simplicity
Deterministic will map state to action pi: s -> a
Stochastic a ~ pi(s, a) = P[a|s]
Avoid storing additional data which may or may not be useful.

2) Stochastic Policies
Learn true stochastic policies
One advantage of Policy based methods over value based methods is that they can learn true stochastic policies.

3) Continuous Action Space

3.3.1 Policy Gradient Methods
Policy Based methods: Search directly for the optimal policy
Policy Gradient methods It is a subset of policy-Based methods. It estimates the best weights by gradient ascent.



3.3.4 Problem Setup
Trajectory: State-action sequence: (s0, a0, r1), (s1, a1, r2), (s2, a2, r3), .....
Loop
    Collect an episode
    Change the weights of the policy network:
        - If WON, increase the probability of each (state, action) combination.
        - If LOST, decrease the probability of each (state, action) combination.


3.4 Proximal Policy Optimization
3.4.2 REINFORCE Review
Here, we briefly review key ingredients of the REINFORCE algorithm.

REINFORCE works as follows: First, we initialize a random policy and using the policy we collect a trajectory -- or a list of (state, actions, rewards) at each time step:
Second, we compute the total reward of the trajectory, and compute an estimate the gradient of the expected reward, 
Third, we update our policy using gradient ascent with learning rate 
The process then repeats.

What are the main problems of REINFORCE? There are three issues:
1) The update process is very inefficient! We run the policy once, update once, and then throw away the trajectory.
2) The gradient estimate g is very noisy. By chance the collected trajectory may not be representative of the policy.
3) There is no clear credit assignment. A trajectory may contain many good/bad actions and whether these actions are reinforced depends only on the final total output.

In the following concepts, we will go over ways to improve the REINFORCE algorithm and resolve all 3 issues. All of the improvements will be utilized and implemented in the PPO algorithm.

4.4.4 Noise Reduction
Gradients are noisy! Sample more trajectories using distributed computing collect the trajectories in parallel. Then calculate the policy gradient using all the different trajectories. Also find the distribution of the rewards by normalizing the rewards.


4.4.5 Credit Assignment
How to modify reward function so that we can differentiate good vs bad actions within a trajectory?
Action to reward: Ignore past reward and consider only future reward for gradient calculations

4.4.9 Importance sampling
Re-weighting factor


4.4.10 Proximal Policy Optimization
Part1: The surrogate function: 

4.4.10 Proximal Policy Optimization
Part2: Clipping Policy Update


4.4.12: PPO Summary
1. Collect trajectories based on pi(theta), initialize theta' = theta
2. Compute the gradient delta(theta')L_clip_sur(theta', theta)
3. Update theta' = theta' + alpha * delta(theta')L_clip_sur(theta', theta)
4. Repeat step2 and 3 a couple times
5. set theta = theta', repeat from step 1


3.5 Actor-Critic Methods
------------------------
3.5.2 Motivation
If you train a neural network to approximate a value function and then use it as a baseline, would this make for a better baseline, and if so, would a better baseline further reduce the variance of policy-based methods? Indeed. In fact, that's basically all actor-critic methods are trying to do, to use value-based techniques to further reduce the variance of policy-based methods. 


3.5.4 Two ways for estimating expected returns
1) Monte-Carlo estimate
Monte-Carlo estimate consists of rolling out an episode in calculating the discounted total reward from the rewards sequence. MC adds all the rewards whether discounted or not. With different episodes some of them will have trajectories that go through the same states. Each of these can give a different MC estimate for the same value function. To calculate the value function, you average the estimates. More estimates the better the value function. 

2) Temporal Difference estimate
TD, to estimate the value of the current state, it uses a single reward sample. In an estimate of the discounted total return, the agent will obtain the next state onwards. So you are estimating with an estimate. This is done using boot strapping in dynamic programming. This basically means that you can leverage the estimate you currently have for the next state in order to calculate a new estimate for the value function of the current state. 

MC estimates will have high variance because estimates for a state can vary greatly across episodes. This is because we are compounding lots of random events that happened during the course of a single episode. But MC are unbiased. You are not estimating using estimates. You are only using true rewards you obtained. So given lots of data, your estimates would be accurate. 

TD estimates are low variance because you are only compounding a single time step of randomness instead of a full rollout. Though because you are bootstrapping on the next state estimates and those are not true values, you are adding bias into your calculations. Your agent will learn faster, but we will have more problems converging.

3.5.5 Baselines and Critics
MC -> unbiased but high variance (DQN) - Policy based - actor
TD -> biased   but low variance (REINFORCE) - Value based - Critic


The return G was calculated as the total discounted return. This way of calculating G is simply a MC estimate that has high variance. Well you then use a baseline to reduce the variance of the reinforce algorithm. However, this baseline was also calculated using the MC approach.

Let's now assume you use deep learning to learn these basics. Even if you still use the MC approach which has high variance, using functional approximation still gives you an advantage. Namely, you now gain the power of generalization. That means when you encounter a new state S', whether you have visited or not, your deep neural network will potentially come up with better estimates, since it's been trained to generalize from similar data. Note that at this point, we're still not using a critic even though we are using functional approximation. This might be confusing as the literature is often not consistent. Through recall, a MC estimate has low bias and high variance, and the TD estimate is biased but low variance. Now the work of critics implies that bias has been introduced and the MC estimate is unbiased. If instead of using MC estimates to train baselines, we use TD estimates, then we can say we have a critic. 

We will be introducing bias but we will be reducing variance thus improving our convergence properties and speeding up learning. In actor-critic methods, all we are trying to do is to continue to reduce the high-variance commonly associated with policy based agents. By using a TD critic instead of a MC baseline, we further reduce the variance of policy-based methods. 

These lead to faster learning than policy-based agents alone and we also see better and more consistent convergence than value-based agents alone. 

3.5.7 A Basic Actor-Critic Agent
An actor-critic agent is an agent that uses function approximation to learn a policy and a value function. We will use two neural networks; One for the actor and one for the critic. The critic will learn to evaluate the state action function V(Pi) using TD estimate. Using the criterion, we will calculate the advantage function and train the actor using this value. A very basic online actor-critic agent is as follows.
You have two networks. One network, the actor, takes in a state and outputs the distribution over actions. The other network, the critic, takes in a state and outputs a state value function of policy Pi, V(Pi). The algorithm goes like this. 

Input the current state into the actor and get the action to take in the state. Observe the next state and reward to get your experienced double (s, a, r, s'). Then, using the TD estimate which is their reward r plus the critic's estimate for s'. So, r plus gamma times V of s', you train the critic.

Next, to calculate the advantage A(s, a) = r + gamma * V(s') - V(s), we also use the critic. Finally, we train the actor using the calculated advantage as a baseline.  

state(s) -> Actor ->  distribution over actions (pi(a|s; theta_pi))

state(s) -> Critic ->  state value function of policy pi v_pi  (V(s; theta_v)

3.5.8 A3C: Asynchronous Advantage Actor-Critic, N-step Bootstrapping
We will be calculating the advantage function APi(s, a) and the critic will be learning to estimate VPi to help with that just as before. If you are using images as inputs to your agent, A3C can use a single CNN with the actor and critic sharing weights, in two separate heads, one for the actor, and one for the critic. Note that A3C is not used exclusively with CNN's and images. But if you were to use it, sharing weights to some more efficient, more complex approach, and can be harder to train. It's a good idea to start with two separate networks, and change it only to improve performance. Now, one interesting aspect of A3C, is that instead of using a TD estimate, it will use another estimate commonly referred to as n-step bootstrapping. N-step bootstrapping, is simply an abstraction and a generalization of a TD and Monte-Carlo estimates. 

TD is a one-step bootstrapping. Your agent goes out and experiences on-time-step real rewards, and then bootstraps right there. Monte-Carlo goes out all the way, and it does not bootstrap because it doesn't need to. MC estimate is an infinite steep bootstrapping. But how about going more than on step, but not all the way out? Can we do two-time steps of real reward, and then bootstrap from the second next state? Can we do three or more? This is what is called n-step bootstrapping, and A3C uses this return to train the critic. For example, on our tenancy example, n-step bootstrapping means that you wait a little bit before guessing what the final score will look like. Waiting to experience the environment for a little longer before you calculate the expected return of the original state, allows you to have less bias in your prediction, keeping variance under control. In practice, only a few steps out, say four or five steps bootstrapping, are often the best. By using n-step bootstrapping, A3C propagates values to the last end states visited, which allows for faster convergence with less experience required while still keeping variance under control.

3.5.9 A3C: Asynchronous Advantage Actor-Critic, Parallel Training
Unlike in DQN, A3C does not use a replay buffer. The main reason we needed a replay buffer was so that we could decorrelate experienced topple. Let me explain.
In RL, an agent collects experience in a sequential manner. The experience collected at time step t + 1 will be correlated to the experience collected at time step t because it is action taken at time step t that is partially responsible for the reward and the state up served at time t + 1, and that state will influence all our further decisions. The replay buffer allows us to collect these experiences sequentially by adding them one at a time to the reply buffer for later processing. So, independent from the data collection process, we can randomly select experiences from the replay buffer into mini batches. The experiences in the mini batches will not show the same correlation. This allows us to train our network successfully. Interestingly, A3C replaces this replay buffer with parallel training. By creating multiple instances of the same environment and and agent running them all at the same time, your agent will receive mini batches of the correlated experiences just as we need. Samples will be decorrelated because agents will likely be experiencing different states at any given time. On top of that, these way of training allows us to use ON policy learning in our learning algorithm which is often associated with more stable learning. 

3.5.10 A3C Asynchronous Advantage Actor-Critic, Off-vs On-policy
On-policy learning is when the policy used for interacting with the environment is also the policy being learned. Off-policy learning is when the policy used for interacting with the environment is different from the policy being learned. SARSA is a good example of an on-policy learning agent. A SARSA agent uses the same policy to interact with the environment as the policy it is learning. On the other hand, Q-learning is a good example of an off-policy learning agent. A Q-learning agent learns about the optimal policy, though the policy that generates behavior is an exploratory policy, often epsilon greedy.

SARSA
Q(S, A) <- Q(S,A) + alpha[R + gamma * Q(S', A') - Q(S, A)]
In SARSA the action used for calculating the TD target and TD Error is the action the agent will take in the following time step A'.

Q-Learning
Q(S, A) <- Q(S,A) + alpha[R + gamma * maxaQ(S', a) - Q(S, A)]
In Q learning however, the action used for calculating the target is the action with the highest value. But this action is not guaranteed to be used by the agent for interaction with the environment in the following time step. In other words, this is not necessarily A'. The Q-learning agent may choose an exploratory action in the next step. In SARSA that action exploratory or not has already been chosen. Q-learning learns the deterministic optimal policy even if its behaviour policy is totally random, SARSA learns the best exploratory policy, that is the best policy that still explores. 

DQN is also an off-policy learning method, your agent behaves with some exploratory policy, say epsilon greedy, where they learn about the optimal policy. When using off-policy learning, agents are able to learn from many different sources including experiences generated by all versions of the agent itself, thus the replay buffer. However, off-policy learning is known to be unstable and often diverge with deep neural networks. A3C on the other hand is a On-policy learning method. With on-policy learning, you only use the data generated by the policy currently being learned about, and anytime you improve the policy, you toss out old data and go out to collect some more. On-policy learning is a bit inefficient in the use of experiences, but it often has more stable and consistent convergence properties. 


On-policy
1. Policy used for interacting with the environment is also the policy being learned
2. SARSA
4. In SARSA the action used for calculating the TD target and TD Error is the action the agent will take in the following time step A'
5. In SARSA that action exploratory or not has already been chosen.
6. SARSA learns the best exploratory policy, that is the best policy that still explores.
7, A3C on the other hand is a On-policy learning method. With on-policy learning, you only use the data generated by the policy currently being learned about, and anytime you improve the policy, you toss out old data and go out to collect some more.
8. On-policy learning is a bit inefficient in the use of experiences, but it often has more stable and consistent convergence properties. 


Off-policy
1. policy used for interacting with the environment is different that the policy being learned
2. Q-Learning, DQN
3. Learns about the optimal policy, though the policy that generates behavior is an exploratory policy, often epsilon greedy
4. In Q learning however, the action used for calculating the target is the action with the highest value. But this action is not guaranteed to be used by the agent for interaction with the environment in the following time step.
5. The Q-learning agent may choose an exploratory action in the next step.
6. Q-learning learns the deterministic optimal policy even if its behaviour policy is totally random
7. Off-policy learning, agents are able to learn from many different sources including experiences generated by all versions of the agent itself thus the replay buffer. 
8. However, off-policy learning is known to be unstable and often diverge with deep neural networks.


3.5.11: A2C Advantage Actor-Critic
What is the asynchronous part in A3C about?
Asynchronous Advantage Actor-Critic. A3C accumulates gradient updates and applies those updates asynchronously to a global neural network. Each agent in the simulation does this at its own time. So, the agents use a local copy of the network to collect experience, calculate, and accumulate gradient updates across multiple time steps, and then they apply these gradients to a global network asynchronously. A synchronous here means that each agent will update the network on its own. There is no synchronization between the agents. This also means that the weights an agent is using might be different from the weights in use by another agent at any given time. There is a synchronous implementation of A3C called Advantage Actor-Critic, A2C. 

A2C has some extra bit of code that synchronizes all agents. It waits for all agents to finish a segment of interaction with its copy of the environment, and then updates the network at once, before sending the updated weights back to all agents. A2C is arguably simpler to implement, yet it gives pretty much the same result, and allegedly in some cases performs even better. 

A3C is most easily trained on a CPU, while A2C is more straightforward to extend on a GPU implementation. 

3.5.13: GAE: Generalized Advantage Estimation
There is another way for estimating expected returns called the lambda return. 
Say after you try and step bootstrapping you realize that numbers of n larger than one often perform better. But it's still hard to tell what the number should be. Should it be a two, three, six or something else. To make the decision even more difficult, in some problems small numbers of n are better while in others, large numbers of n are better. How do we get this right? The idea of the lambda return is to create a mixture of all n-step bootstrapping estimates out once. Lambda is a hyperparameter used for weighting the combination of each n-step estimate to the lambda return.

(1-lambda), (1-lambda)*lambda, (1-lambda)*lambda^2, (1-lambda)*lambda^n, lambda^(T-t-1). The contribution to the lambda return would be a combination of all n-step returns weighted by the exponentially decaying factor across the different n-step estimates. Notice how the weight depends on the value of lambda you set and it decays exponentially at the rate of that value. So, for calculating the lambda return for state s at time step t, we would use all n-steps returns and multiply each of the n-step returns by the corresponding weight. Then add them all up. The sum will be the lambda return for state s at time step t. If, lambda is set to zero, the two-step,  all except 1 step would be equal to zero. So, the lambda return when lambda is set to zero will be equivalent to the td estimate (One-step bootstrapping TD estimate). If lambda is set to one all n-step return other than the infinite step return will be set to zero. So, the lambda return when lambda is set to one, will be equivalent to the Monte Carlo estimate. Again, a number between zero and one gives a mixture of all n-step bootstrapping estimates. A single algorithm can do that. 
GAE is a way to train the critic with this lambda return. You can fit the advantage function just like in A3C and A2C or using a mixture of n-step bootstrapping estimates. Its important to highlight that this type of return can be combined with virtually any policy-based method. In fact, in the paper that introduced GAE, TRPO was that policy-based method used. By using this type of estimation, this algorithm, TRTO plus GAE trains very quickly because multiple value functions are spread around on every time step due to the lambda return star estimate. 

3.5.14 DDPG: Deep Deterministic Policy Gradient Continuous Actions
It could be seen as an approximate DQN, instead of an actual actor critic. The reason for this is that the critic in DDPG, is used to approximate the maximizer over the Q values of the next state, and not as a learned baseline, as we have seen so far. Though, this is still a very important algorithm and it is good to discuss it in more detail. 

One of the limitations of the DQN agent is that it is not straightforward to use in continuous action spaces. It's a discrete action space. But what if you need an action which is continuous. How do you get the value of a continuous action with this architecture? Say a variable between 1 and 100 centimeters. This is one of the problems DDPG solves. 

In DDPG, we use two deep neural networks. One the actor and the other the critic. Now the actor here is used to approximate the optimal policy deterministically. That means we want to always output the best believed action for any given state. This is unlike a stochastic policy in which we want the policy to learn a probability distribution over the actions. In DDPG, we want the best action every single time we query the actor network. That is a deterministic policy. The actor is basically learning the argmax_a Q(S,A) which is the best action. The critic learns to evaluate the optimal action value function by using the actions best believed action. Again, we use this actor, which is an approximate maximizer, to calculate a new target value for training the action value function, much in the way DQN does. 

3.5.15 DDPG: Deep Deterministic Policy Gradient, Soft Updates
Two other interesting aspects of DDPG are 
1) Use of replay buffer
2) Soft updates of the target networks. 
In DQN, you have two copies of your network weights, the regular and the target network. In the Atari paper in which DQN was introduced, the target network is updated every 10,000 time steps. You simply copy the weights of your regular network into your target network. That is the target network is fixed for 10,000 time steps and then it gets a big update. In DDPG, you have two copies of your network weights for each network, a regular for the actor, and regular for the critic and a target for the actor, and a target for the critic. But in DDPG, the target networks are updated using a soft updates strategy. A soft update strategy consists of slowly blending your regular network weights with your target network weights. So, every time step you make your target network be 99.99 percent of your target network weights and only a 0.01 percent of your regular network weights. You are slowly mixing in your regular network weights into your target network weights. Recall, the regular network is the most up to date network because it's the one we are training, while the target network is the one we use for prediction to stabilize training. In practice, you will get faster convergence by using this update strategy, and in fact, this way for updating the target network weights can be used with other algorithms that use target networks including DQN.  























1. Reinforcement Learning (RL)

Goal:Learn a policy that maximizes expected cumulative reward.
Setup:The agent explores an environment, receives state â†’ action â†’ reward feedback, and improves its behavior over time.
Key Point:You must define the reward function(what you want the robot to optimize).
Example:Training a humanoid robot to walk by giving positive rewards for forward motion and penalties for falling.

---

2. Inverse Reinforcement Learning (IRL)

Goal:Instead of defining the reward function, you infer it from expert demonstrations.
Setup:You observe an expert (e.g., a human or another robot) performing tasks. Then, you try to recover the reward function that explains the expertâ€™s behavior as being optimal.
Key Point:IRL is about understanding the â€œwhyâ€behind expert actions. Once the reward is learned, you can apply RL to learn the policy.
Example:Watching how a surgeon manipulates instruments and inferring the underlying objective (e.g., minimizing tissue damage + task completion).

---

3. Imitation Learning (IL)

Goal:Directly learn a policy from expert demonstrations without explicitly recovering the reward.
Setup:You treat expert demonstrations as supervised data: input = state, output = expertâ€™s action. The agent just copies the behavior.
Key Point:IL skips reward inference and is usually faster to train, but it may fail when the agent encounters unseen states (distribution shift).
Example:A humanoid robot learns to pour tea by directly mapping visual observations to actions, imitating a human demonstrator.

---

ðŸ”‘ Summary

RL: Learn from scratch using rewards. (Needs a reward function)
IRL: Learn the reward function from demonstrations. (Focus on â€œwhyâ€)
IL: Learn the policy directly from demonstrations. (Focus on â€œhowâ€)

---

ðŸ‘‰ Quick analogy:

RL is like training a dog with treats (explicit reward).
IRL is like figuring out why a dog runs to the door when the bell rings (inferring the hidden reward).
IL is like copying someoneâ€™s dance moves directly (no reward, just mimic).

##################################

â€œLet me explain the difference between Reinforcement Learning, Inverse Reinforcement Learning, and Imitation Learning using a humanoid example â€” teaching the robot to pick up a cup and hand it to a person.

With **Reinforcement Learning**, I would design a reward function: the robot gets points for touching the cup, grasping it, and successfully handing it over, and penalties if it drops it. The robot then learns through trial and error to maximize these rewards.

With **Inverse Reinforcement Learning**, instead of writing rewards myself, Iâ€™d let the robot watch a human do the task multiple times. The robot then infers what reward function the human seems to be following â€” like being gentle, safe, and smooth â€” and uses that to guide its own learning.

With **Imitation Learning**, the robot simply mimics the demonstrated behavior. I record how a human reaches, grasps, and hands over the cup, and the robot directly learns to map its observations to those same actions.

So in short: RL is about learning from trial and error with explicit rewards, IRL is about learning the hidden rewards from human demonstrations, and IL is about directly copying human actions. Each is useful depending on whether I can define rewards easily, need to capture human intent, or just want quick learning from demos.â€

######################################
â€œLet me explain the difference between Reinforcement Learning, Inverse Reinforcement Learning, and Imitation Learning using a humanoid example â€” teaching the robot to pick up a cup and hand it to a person.

With **Reinforcement Learning**, I would design a reward function: the robot gets points for touching the cup, grasping it, and successfully handing it over, and penalties if it drops it. The robot then learns through trial and error to maximize these rewards.

With **Inverse Reinforcement Learning**, instead of writing rewards myself, Iâ€™d let the robot watch a human do the task multiple times. The robot then infers what reward function the human seems to be following â€” like being gentle, safe, and smooth â€” and uses that to guide its own learning.

With **Imitation Learning**, the robot simply mimics the demonstrated behavior. I record how a human reaches, grasps, and hands over the cup, and the robot directly learns to map its observations to those same actions.

So in short: RL is about learning from trial and error with explicit rewards, IRL is about learning the hidden rewards from human demonstrations, and IL is about directly copying human actions. For humanoids, RL is often best for locomotion and balance, IL works well for simple manipulation tasks, and IRL is powerful when we need the robot to respect human comfort and intent in shared environments.â€
################################################